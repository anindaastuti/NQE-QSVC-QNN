{"cells":[{"cell_type":"markdown","metadata":{"id":"zyS1erbo57-5"},"source":["## Importing necessary modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DbqSqNWY57-8"},"outputs":[],"source":["import os\n","import io\n","import glob\n","import time\n","import shutil\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","\n","from itertools import product\n","\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from qiskit import QuantumCircuit\n","from qiskit.circuit import Parameter, ParameterVector\n","from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n","\n","from qiskit_machine_learning.utils import algorithm_globals\n","from qiskit_machine_learning.optimizers import COBYLA, L_BFGS_B\n","from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier, VQC\n","from qiskit_machine_learning.algorithms.regressors import NeuralNetworkRegressor, VQR\n","from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n","from qiskit_machine_learning.circuit.library import QNNCircuit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bcy4nKB857--"},"outputs":[],"source":["print(torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(torch.cuda.get_device_name())\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"cyTTVyN557--"},"source":["## Random seed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRbthVp_57-_"},"outputs":[],"source":["def same_seeds(seed):\n","    # Python built-in random module\n","    # random.seed(seed)\n","    # Numpy\n","    np.random.seed(seed)\n","    # Torch\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","    # qiskit\n","    algorithm_globals.random_seed = seed\n","\n","RANDOM_SEED = 0\n","same_seeds(RANDOM_SEED)"]},{"cell_type":"markdown","metadata":{"id":"mNwqVfqr57-_"},"source":["## Directory paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-GOeQOdx57-_"},"outputs":[],"source":["# Define the directory paths for the results\n","sub_dirname = '4PCs_QNN_KFold_best_loss'\n","result_dir = os.path.join(\"./results\", sub_dirname)\n","results_dir = {\n","    'figures': os.path.join(result_dir, 'figures'),\n","    'logs': os.path.join(result_dir, 'logs'),\n","    'hyperparameters': os.path.join(result_dir, 'hyperparameters'),\n","    'metrics': os.path.join(result_dir, 'metrics'),\n","    'checkpoints': os.path.join(result_dir, 'checkpoints'),\n","}\n","\n","# Remove the existing directories and create new ones\n","for key in results_dir.keys():\n","    if isinstance(results_dir[key], dict):\n","        for k in results_dir[key].keys():\n","            if os.path.exists(results_dir[key][k]):\n","                shutil.rmtree(results_dir[key][k])\n","            os.makedirs(results_dir[key][k])\n","    else:\n","        if os.path.exists(results_dir[key]):\n","            shutil.rmtree(results_dir[key])\n","        os.makedirs(results_dir[key])"]},{"cell_type":"markdown","metadata":{"id":"6e2T9AAt57-_"},"source":["## Load the 4PCs dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jgrAIbnc57-_"},"outputs":[],"source":["# load the dataset\n","# feature_1, feature_2, feature_3, feature_4, label\n","train_data_path = './data/X_train__w_4PCs.xlsx'\n","test_data_path = './data/X_test_w_4PCs.xlsx'\n","train_data = pd.read_excel(train_data_path)\n","test_data = pd.read_excel(test_data_path)\n","\n","# rename the columns\n","columns = ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'label']\n","train_data.columns = columns\n","test_data.columns = columns\n","\n","# display the dataset information and statistics\n","print(f\"Train data shape: {train_data.shape}\")\n","print(f\"Test data shape: {test_data.shape}\")\n","print(\"-\"*50)\n","\n","print(f\"Train data class distribution: \\n{train_data['label'].value_counts()}\")\n","print(\"-\"*50)\n","print(f\"Test data class distribution: \\n{test_data['label'].value_counts()}\")\n","print(\"-\"*50)\n","\n","print(f\"Train data description: \\n{train_data.describe()}\")\n","print(f\"Test data description: \\n{test_data.describe()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBzMLsy657_A"},"outputs":[],"source":["# visualize the train data\n","print(f'Train data: {train_data.shape}')\n","sns.pairplot(train_data, hue=\"label\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5c0g1kY57_A"},"outputs":[],"source":["# visualize the test data\n","print(f'Test data: {test_data.shape}')\n","sns.pairplot(test_data, hue='label')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"EsO2Xwdn57_A"},"source":["## Models"]},{"cell_type":"markdown","metadata":{"id":"yOuLHtEM57_A"},"source":["### Example of Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkTm4cc_57_B"},"outputs":[],"source":["# Example of Parameters\n","num_qubits_example = 4\n","feature_map_reps_example = 2\n","ansatz_reps_example = 2\n","loss_fn_example = 'cross_entropy'\n","optimizer_example = COBYLA(maxiter=100)"]},{"cell_type":"markdown","metadata":{"id":"d1OjyCxn57_B"},"source":["### Interpret Method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QRsfR94e57_B"},"outputs":[],"source":["# parity maps bitstrings to 0 or 1\n","def parity(x):\n","    return \"{:b}\".format(x).count(\"1\") % 2"]},{"cell_type":"markdown","metadata":{"id":"7vP6XbTk57_B"},"source":["### VQC Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3HMWsdhK57_B"},"outputs":[],"source":["from qiskit.primitives import Sampler\n","\n","def create_VQC_model(num_qubits, feature_map_reps, ansatz_reps, loss_fn, optimizer, callback):\n","    # Create the feature map and ansatz\n","    feature_map = ZZFeatureMap(feature_dimension=num_qubits, reps=feature_map_reps)\n","    ansatz = RealAmplitudes(num_qubits=num_qubits, reps=ansatz_reps)\n","    # Create the sampler\n","    sampler = Sampler()\n","\n","    # Create the VQC model\n","    vqc = VQC(\n","        sampler=sampler,\n","        feature_map=feature_map,\n","        ansatz=ansatz,\n","        loss=loss_fn,\n","        optimizer=optimizer,\n","        callback=callback,\n","    )\n","\n","    return vqc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-HStUG5J57_B"},"outputs":[],"source":["# Example of the VQC model\n","circuit = create_VQC_model(\n","    num_qubits=num_qubits_example,\n","    feature_map_reps=feature_map_reps_example,\n","    ansatz_reps=ansatz_reps_example,\n","    loss_fn=loss_fn_example,\n","    optimizer=optimizer_example,\n","    callback=None\n",").circuit\n","circuit.decompose().draw(output=\"mpl\", style=\"clifford\", fold=20)"]},{"cell_type":"markdown","metadata":{"id":"jg4OVmjP57_C"},"source":["### SamplerQNN Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXsw3V5A57_C"},"outputs":[],"source":["from qiskit.primitives import Sampler\n","\n","def create_SamplerQNN_model(num_qubits, feature_map_reps, ansatz_reps, loss_fn, optimizer, callback):\n","    # Create the feature map and ansatz\n","    feature_map = ZZFeatureMap(feature_dimension=num_qubits, reps=feature_map_reps)\n","    ansatz = RealAmplitudes(num_qubits=num_qubits, reps=ansatz_reps)\n","\n","    # Create the quantum circuit\n","    qc = QuantumCircuit(num_qubits)\n","    qc.compose(feature_map, inplace=True)\n","    qc.compose(ansatz, inplace=True)\n","\n","    # Create the sampler\n","    sampler = Sampler()\n","\n","    # Define the output shape\n","    output_shape = 2 # parity maps bitstrings to 0 or 1\n","\n","    # Create the QNN model\n","    sampler_qnn = SamplerQNN(\n","        circuit=qc,\n","        input_params=feature_map.parameters,\n","        weight_params=ansatz.parameters,\n","        interpret=parity,\n","        output_shape=output_shape,\n","        input_gradients=False\n","    )\n","\n","    classifier = NeuralNetworkClassifier(\n","        neural_network=sampler_qnn,\n","        optimizer=optimizer,\n","        callback=callback,\n","        loss=loss_fn,\n","        one_hot=True,\n","    )\n","\n","    return classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTXpHahs57_C"},"outputs":[],"source":["# Example of SampleQNN model\n","circuit = create_SamplerQNN_model(\n","    num_qubits=num_qubits_example,\n","    feature_map_reps=feature_map_reps_example,\n","    ansatz_reps=ansatz_reps_example,\n","    loss_fn=loss_fn_example,\n","    optimizer=optimizer_example,\n","    callback=None\n",").neural_network.circuit\n","circuit.decompose().draw(output=\"mpl\", style=\"clifford\", fold=20)"]},{"cell_type":"markdown","metadata":{"id":"spbFa4ib57_C"},"source":["### SamplerQNN Model with Custom FeatureMap and Ansatz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9G3sqAU57_C"},"outputs":[],"source":["from qiskit.primitives import Sampler\n","\n","def custom_feature_map(num_qubits, reps):\n","    inputs = ParameterVector(\"input\", num_qubits)\n","    qc = QuantumCircuit(num_qubits)\n","\n","    for i in range(reps):\n","        for j in range(num_qubits):\n","            qc.h(j)\n","            qc.ry(inputs[j], j)\n","\n","    return qc\n","\n","def custom_ansatz(num_qubits, reps):\n","    weights = ParameterVector(\"weight\", num_qubits * reps)\n","    qc = QuantumCircuit(num_qubits)\n","\n","    for i in range(reps):\n","        for j in range(num_qubits-1):\n","            qc.cx(j, j + 1)\n","        qc.cx(num_qubits-1, 0)\n","        qc.barrier()\n","        for j in range(num_qubits):\n","            qc.ry(weights[i*num_qubits + j], j)\n","\n","    return qc\n","\n","def create_custom_SamplerQNN_model(num_qubits, feature_map_reps, ansatz_reps, loss_fn, optimizer, callback):\n","    # Create the feature map and ansatz\n","    feature_map = custom_feature_map(num_qubits, feature_map_reps)\n","    ansatz = custom_ansatz(num_qubits, ansatz_reps)\n","\n","    # Create the quantum circuit\n","    qc = QuantumCircuit(num_qubits)\n","    qc.compose(feature_map, inplace=True)\n","    qc.compose(ansatz, inplace=True)\n","\n","    # Create the sampler\n","    sampler = Sampler()\n","\n","    # Define the output shape\n","    output_shape = 2 # parity maps bitstrings to 0 or 1\n","\n","    # Create the QNN model\n","    sampler_qnn = SamplerQNN(\n","        circuit=qc,\n","        input_params=feature_map.parameters,\n","        weight_params=ansatz.parameters,\n","        interpret=parity,\n","        output_shape=output_shape,\n","        input_gradients=False\n","    )\n","\n","    # Create the classifier\n","    classifier = NeuralNetworkClassifier(\n","        neural_network=sampler_qnn,\n","        optimizer=optimizer,\n","        callback=callback,\n","        loss=loss_fn,\n","        one_hot=True,\n","    )\n","\n","    return classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w7jmYQrP57_D"},"outputs":[],"source":["# Example of the custom model\n","circuit = create_custom_SamplerQNN_model(\n","    num_qubits=num_qubits_example,\n","    feature_map_reps=feature_map_reps_example,\n","    ansatz_reps=ansatz_reps_example,\n","    loss_fn=loss_fn_example,\n","    optimizer=optimizer_example,\n","    callback=None\n",").neural_network.circuit\n","circuit.decompose().draw(output=\"mpl\", style=\"clifford\", fold=20)"]},{"cell_type":"markdown","metadata":{"id":"_DrOxOh257_D"},"source":["## Analysis of Validation Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2p42IUzV57_D"},"outputs":[],"source":["from typing import List\n","\n","def compute_confusion_matrix_values(confusion):\n","    \"\"\"\n","    Compute the TP, FP, FN, and TN values from the confusion matrix.\n","    Parameters:\n","        confusion (np.ndarray): Confusion matrix of shape (n_classes, n_classes).\n","    Returns:\n","        tuple: Tuple containing TP, FP, FN, and TN values.\n","    \"\"\"\n","    if confusion.shape == (2, 2):\n","        \"\"\"\n","        [\n","            [TN, FP]\n","            [FN, TP]\n","        ]\n","        \"\"\"\n","        TN, FP, FN, TP = confusion.ravel()\n","    else:\n","        # True Positive: The number of samples that the model correctly predicts as class i\n","        TP = np.diag(confusion)\n","        # False Positive: The number of samples that the model incorrectly predicts as class i\n","        FP = confusion.sum(axis=0) - TP\n","        # False Negative: The number of samples that the model incorrectly predicts as not class i\n","        FN = confusion.sum(axis=1) - TP\n","        # True Negative: The number of samples that the model correctly predicts as not class i\n","        TN = confusion.sum() - (TP + FP + FN)\n","    return TP, FP, FN, TN\n","\n","def calculate_metrics(confusion, average=None, weighted=None):\n","    \"\"\"\n","    Calculate various classification metrics based on the provided confusion matrix.\n","    Parameters:\n","        confusion (np.ndarray): Confusion matrix of shape (n_classes, n_classes).\n","        average (str, optional): Type of averaging to be performed on the data.\n","                                Options are 'micro', 'macro', 'weighted', or None. Default is None.\n","        weighted (list or np.ndarray, optional): Weights for each class if average is 'weighted'.\n","                                                Must be provided if average is 'weighted'.\n","    Returns:\n","        dict: Dictionary containing the calculated metrics.\n","    \"\"\"\n","    # Check the input arguments\n","    if isinstance(confusion, List):\n","        confusion = np.array(confusion) # Convert the list to a numpy array\n","    if not isinstance(confusion, np.ndarray):\n","        raise TypeError(\"The confusion matrix must be a numpy array.\")\n","    if len(confusion.shape) != 2 or confusion.shape[0] != confusion.shape[1]:\n","        raise ValueError(\"The confusion matrix must be a square matrix.\")\n","    if average not in [None, 'macro', 'micro', 'weighted']:\n","        raise ValueError(\"The average must be one of 'micro', 'macro', 'weighted', or None.\")\n","    if average == 'weighted' and weighted is None:\n","        raise ValueError(\"The weights must be provided if average is 'weighted'.\")\n","    if weighted is not None and len(confusion) != len(weighted):\n","        raise ValueError(\"The number of classes and the length of the weights must be the same.\")\n","\n","    # Compute the TP, FP, FN, and TN values\n","    TP, FP, FN, TN = compute_confusion_matrix_values(confusion)\n","\n","    # If the average is 'micro', sum the values\n","    if average == 'micro':\n","        TP, FP, FN, TN = map(np.sum, [TP, FP, FN, TN])\n","\n","    def safe_divide(numerator, denominator):\n","        \"\"\"Safely divide two numbers, returning 0 if the denominator is 0.\"\"\"\n","        return np.divide(numerator, denominator, out=np.zeros_like(numerator, dtype=float), where=denominator!=0)\n","\n","    # Initialize the metrics dictionary\n","    metrics = {}\n","\n","    # Accuracy (ACC)\n","    accuracy = safe_divide(TP + TN, TP + TN + FP + FN)\n","    # Precision or Positive Predictive Value (PPV)\n","    precision = safe_divide(TP, TP + FP)\n","    # Recall or Sensitivity or True Positive Rate (TPR)\n","    recall = safe_divide(TP, TP + FN)\n","    # F1 Score\n","    f1 = safe_divide(2 * (precision * recall), precision + recall)\n","    # Matthews Correlation Coefficient (MCC)\n","    mcc = safe_divide((TP * TN - FP * FN), np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)))\n","    # Negative Predictive Value (NPV)\n","    npv = safe_divide(TN, TN + FN)\n","    # True Negative Rate (TNR) or Specificity (SPC)\n","    tnr = safe_divide(TN, TN + FP)\n","    # False Positive Rate (FPR)\n","    fpr = safe_divide(FP, TN + FP)\n","    # False Discovery Rate (FDR)\n","    fdr = safe_divide(FP, TP + FP)\n","    # False Negative Rate (FNR)\n","    fnr = safe_divide(FN, TP + FN)\n","\n","    # Create a dictionary of all metrics\n","    metrics = {\n","        'TP': TP,\n","        'FP': FP,\n","        'FN': FN,\n","        'TN': TN,\n","        'TPR': recall,\n","        'SPC': tnr,\n","        'PPV': precision,\n","        'F1': f1,\n","        'ACC': accuracy,\n","        'NPV': npv,\n","        'FPR': fpr,\n","        'FDR': fdr,\n","        'FNR': fnr,\n","        'MCC': mcc\n","    }\n","\n","    if average == 'macro':\n","        for metric, values in metrics.items():\n","            metrics[metric] = np.mean(values)\n","    elif average == 'weighted':\n","        for metric, values in metrics.items():\n","            metrics[metric] = np.average(values, weights=weighted)\n","\n","    return metrics"]},{"cell_type":"markdown","metadata":{"id":"K7U-3muE57_D"},"source":["## Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NS6oG7dt57_D"},"outputs":[],"source":["# Define the hyperparameters\n","target_names = ['Class 0', 'Class 1']\n","k_folds = 5 # K-fold cross-validation\n","optimizer_fn = COBYLA\n","loss_fn = 'cross_entropy'\n","average = 'macro'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qt3Eo7K457_D"},"outputs":[],"source":["# Define the search space for the hyperparameters\n","models = ['SamplerQNN']\n","epochs = [50, 100, 200]\n","feature_map_reps_list = [2, 3]\n","ansatz_reps_list = [2, 3]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8hNinW657_E"},"outputs":[],"source":["# Function to create the model based on the parameters\n","def get_create_model_function(model_name):\n","    if model_name == 'VQC':\n","        return create_VQC_model\n","    elif model_name == 'SamplerQNN':\n","        return create_SamplerQNN_model\n","    elif model_name == 'CustomSamplerQNN':\n","        return create_custom_SamplerQNN_model\n","    else:\n","        raise ValueError(f\"Invalid model name: {model_name}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqjs97ju57_E"},"outputs":[],"source":["# Create the parameter combinations\n","param_combinations = list(product(models, epochs, feature_map_reps_list, ansatz_reps_list))\n","\n","print(f\"Number of parameter combinations: {len(param_combinations)}\")\n","for i, params in enumerate(param_combinations):\n","    model_name, epochs, feature_map_reps, ansatz_reps = params\n","    print(f\"Parameter combination {i+1}:\")\n","    print(f\"\\tModel: {model_name}\")\n","    print(f\"\\tEpochs: {epochs}\")\n","    print(f\"\\tFeature map reps: {feature_map_reps}\")\n","    print(f\"\\tAnsatz reps: {ansatz_reps}\")"]},{"cell_type":"markdown","metadata":{"id":"uaZFP5pE57_E"},"source":["## Stratified K-Fold Cross Validation\n","Find the best parameter combinations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fQ1AeXwb57_E"},"outputs":[],"source":["# Split the data into features and labels\n","x_train = train_data.iloc[:, :-1].values\n","y_train = train_data['label'].values\n","x_test = test_data.iloc[:, :-1].values\n","y_test = test_data['label'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0qw7Khw957_F"},"outputs":[],"source":["objective_func_vals = [] # To store the objective function values\n","def callback_recorder(weights, obj_func_eval):\n","        objective_func_vals.append(obj_func_eval)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhTtrHgq57_F"},"outputs":[],"source":["# Initialize the StratifiedKFold\n","skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=RANDOM_SEED)\n","\n","# Iterate over the parameter combinations\n","total_start = time.time() # To store the total time taken for the entire training\n","mean_losses = [] # To store the mean losses for each parameter combination\n","metrics_columns = [] # To store the columns for the metrics\n","metrics_record = [] # To store the metrics for each parameter combination\n","for param_index, param_comb in enumerate(param_combinations):\n","    # Get the parameter combination\n","    model_name, epochs, feature_map_reps, ansatz_reps = param_comb\n","    create_model_fn = get_create_model_function(model_name)\n","    optimizer = optimizer_fn(maxiter=epochs)\n","\n","    print(\"=\"*100)\n","    print(f\"Parameter combination {param_index+1}/{len(param_combinations)}\")\n","    print(f\"Model: {model_name}, Epochs: {epochs}, Feature map reps: {feature_map_reps}, Ansatz reps: {ansatz_reps}\")\n","    print(\"=\"*100)\n","\n","    # Save the hyperparameters configuration to a file\n","    params_names = f'{model_name}_fmr{feature_map_reps}_ar{ansatz_reps}'\n","    with open(os.path.join(results_dir['hyperparameters'], f'hyperparameters_{params_names}.txt'), 'w') as f:\n","        f.write(f\"Optimizer: {optimizer.__class__.__name__}\\n\")\n","        f.write(f\"Loss function: {loss_fn}\\n\")\n","        f.write(f\"Model: {model_name}\\n\")\n","        f.write(f\"Epochs: {epochs}\\n\")\n","        f.write(f\"Feature map reps: {feature_map_reps}\\n\")\n","        f.write(f\"Ansatz reps: {ansatz_reps}\\n\")\n","\n","    # Create the classifier model\n","    num_qubits = x_train.shape[1]\n","    classifier = create_model_fn(num_qubits, feature_map_reps, ansatz_reps, loss_fn, optimizer, callback_recorder)\n","\n","    # Save the circuit diagram\n","    classifier_circuit = None\n","    if hasattr(classifier, 'circuit'):\n","        classifier_circuit = classifier.circuit\n","    else:\n","        classifier_circuit = classifier.neural_network.circuit\n","    classifier_circuit.decompose().draw(output=\"mpl\", style=\"clifford\", fold=20) \\\n","        .savefig(os.path.join(results_dir['figures'], f'circuit_{params_names}.png'))\n","\n","    # Iterate over the folds\n","    fold_losses = [] # To store the losses for each fold\n","    for fold_index, (train_index, val_index) in enumerate(skf.split(x_train, y_train)):\n","        fold_time_start = time.time() # To store the time taken for the current parameter combination\n","\n","        # Display the fold number\n","        print(f\"Fold {fold_index+1}:\")\n","\n","        # Define the model name\n","        fold_name = f\"{params_names}_fold{fold_index+1}\"\n","\n","        # ================================================================================\n","        # Data Preprocessing\n","        # ================================================================================\n","        # Split the data into training and validation sets\n","        x_train_fold, x_val_fold = x_train[train_index], x_train[val_index]\n","        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","\n","        # Normalize the data\n","        scaler = MinMaxScaler()\n","        x_train_fold = scaler.fit_transform(x_train_fold)\n","        x_val_fold = scaler.transform(x_val_fold)\n","\n","        # ================================================================================\n","        # Model Training\n","        # ================================================================================\n","        # Train the model\n","        objective_func_vals = [] # Reset the objective function values\n","        classifier.fit(x_train_fold, y_train_fold)\n","\n","        # Save the model\n","        classifier.save(os.path.join(results_dir['checkpoints'], f'{fold_name}.model'))\n","\n","        # Plot the training loss curve, and save the figure\n","        plt.figure(figsize=(6, 4))\n","        plt.title(f\"Objective function value against iteration - {fold_name}\")\n","        plt.xlabel(\"Iteration\")\n","        plt.ylabel(\"Objective function value\")\n","        plt.plot(range(len(objective_func_vals)), objective_func_vals)\n","        plt.savefig(os.path.join(results_dir['figures'], f'loss_curve_{fold_name}.png'))\n","        plt.show()\n","        plt.close()\n","\n","        # Save the loss values to a file\n","        with open(os.path.join(results_dir['logs'], f'loss_{fold_name}.txt'), 'w') as f:\n","            f.write(\",\".join(map(str, objective_func_vals)))\n","\n","        # Record the final loss value for the fold\n","        fold_losses.append(objective_func_vals[-1])\n","        print(f\"Final loss: {objective_func_vals[-1]}\")\n","\n","        # ================================================================================\n","        # Model Evaluation\n","        # ================================================================================\n","        # Evaluate score on the validation data\n","        val_score = classifier.score(x_val_fold, y_val_fold)\n","        print(f\"Validation score: {val_score}\")\n","\n","        # Calculate and plot the confusion matrix for the validation data\n","        y_pred_val = classifier.predict(x_val_fold)\n","        confusion = confusion_matrix(y_val_fold, y_pred_val)\n","        df_cm = pd.DataFrame(confusion, index=target_names, columns=target_names)\n","        plt.figure(figsize=(6, 4))\n","        sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n","        plt.xlabel(\"Prediction\")\n","        plt.ylabel(\"Ground Truth\")\n","        plt.title(f\"Confusion Matrix - {fold_name}\")\n","        plt.savefig(os.path.join(results_dir['figures'], f'confusion_matrix_{fold_name}.png'))\n","        plt.show()\n","\n","        # Calculate the classification report\n","        print(\"Classification report:\")\n","        print(classification_report(y_val_fold, y_pred_val, target_names=target_names, zero_division=0))\n","\n","        # Calculate the metrics\n","        metrics = calculate_metrics(confusion, average=average)\n","        metrics = {k: np.round(v, 4) for k, v in metrics.items()} # Round the values to 4 decimal places\n","\n","        # Display the metrics\n","        print(f\"Averaging method: {average}\")\n","        print(\"Metrics:\")\n","        for metric, value in metrics.items():\n","            print(f\"\\t{metric}: {value}\")\n","\n","        # Save the parameters and metrics to a file\n","        metrics_columns = ['Model Name', 'Epochs', 'Feature Map Reps', 'Ansatz Reps', 'Fold', 'Loss'] + list(metrics.keys())\n","        metrics_values = [model_name, epochs, feature_map_reps, ansatz_reps, fold_index+1, objective_func_vals[-1]] + list(metrics.values())\n","        metrics_record.append(metrics_values)\n","\n","        # Calculate the time taken for the current fold\n","        fold_time_elapsed = time.time() - fold_time_start\n","        print(f\"Fold time: {round(fold_time_elapsed)} seconds\")\n","\n","    # ================================================================================\n","    # Current parameter combination training is done\n","    # Record the mean validation score for the parameter combination\n","    # ================================================================================\n","\n","    # Calculate the mean validation score for the parameter combination\n","    mean_loss = np.array(fold_losses).mean()\n","    mean_losses.append(mean_loss)\n","    print(f\"Mean validation loss: {mean_loss}\")\n","\n","# ================================================================================\n","# All parameter combinations training is done\n","# ================================================================================\n","total_elapsed = time.time() - total_start\n","print(f\"Total training time: {round(total_elapsed)} seconds\")\n","\n","# Save the metrics to a file\n","metrics_df = pd.DataFrame(metrics_record, columns=metrics_columns)\n","metrics_df.to_csv(os.path.join(results_dir['metrics'], 'all_metrics.csv'), index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oPyyaYg557_F"},"outputs":[],"source":["# Select the best parameter combination\n","best_param_index = np.argmin(mean_losses)\n","best_param_comb = param_combinations[best_param_index]\n","best_model_name, best_epochs, best_feature_map_reps, best_ansatz_reps = best_param_comb\n","best_create_model_fn = get_create_model_function(best_model_name)\n","\n","print(f\"Best model: {best_model_name}\")\n","print(f\"Best epochs: {best_epochs}\")\n","print(f\"Best feature map reps: {best_feature_map_reps}\")\n","print(f\"Best ansatz reps: {best_ansatz_reps}\")\n","print(f\"Best mean loss: {mean_losses[best_param_index]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"seeukWwa57_G"},"outputs":[],"source":["# Save the besthyperparameters configuration to a file\n","best_model_name = f'best_{best_model_name}_fmr{best_feature_map_reps}_ar{best_ansatz_reps}'\n","with open(os.path.join(results_dir['hyperparameters'], f'hyperparameters_{best_model_name}.txt'), 'w') as f:\n","    f.write(f\"Optimizer: {optimizer.__class__.__name__}\\n\")\n","    f.write(f\"Loss function: {loss_fn}\\n\")\n","    f.write(f\"Model: {model_name}\\n\")\n","    f.write(f\"Epochs: {epochs}\\n\")\n","    f.write(f\"Feature map reps: {feature_map_reps}\\n\")\n","    f.write(f\"Ansatz reps: {ansatz_reps}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"4yA-sPMd57_G"},"source":["## Train the Model with Best Parameter Combinations"]},{"cell_type":"markdown","metadata":{"id":"8J4-gkPK57_G"},"source":["### Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jTju8q0o57_H"},"outputs":[],"source":["# Normalize the data\n","scaler = MinMaxScaler()\n","x_train = scaler.fit_transform(x_train)\n","x_test = scaler.transform(x_test)"]},{"cell_type":"markdown","metadata":{"id":"nubXFyaD57_H"},"source":["### Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RhLw1Xe57_H"},"outputs":[],"source":["from IPython.display import clear_output\n","\n","objective_func_vals = []\n","plt.rcParams[\"figure.figsize\"] = (12, 6)\n","\n","# callback function that draws a live plot when the .fit() method is called\n","def callback_graph(weights, obj_func_eval):\n","    clear_output(wait=True)\n","    objective_func_vals.append(obj_func_eval)\n","    plt.title(\"Objective function value against iteration\")\n","    plt.xlabel(\"Iteration\")\n","    plt.ylabel(\"Objective function value\")\n","    plt.plot(range(len(objective_func_vals)), objective_func_vals)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q9c7E-NH57_I"},"outputs":[],"source":["# Create the classifier model\n","num_qubits = x_train.shape[1]\n","objective_func_vals = [] # Reset the objective function values\n","classifier = best_create_model_fn(num_qubits, best_feature_map_reps, best_ansatz_reps, loss_fn, optimizer, callback_graph)\n","\n","# Display the circuit\n","classifier_circuit = None\n","if hasattr(classifier, 'circuit'):\n","    classifier_circuit = classifier.circuit\n","else:\n","    classifier_circuit = classifier.neural_network.circuit\n","\n","classifier_circuit.decompose().draw(output=\"mpl\", style=\"clifford\", fold=20) \\\n","    .savefig(os.path.join(results_dir['figures'], f'circuit_{best_model_name}.png'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jcSTyF4z57_I"},"outputs":[],"source":["# fit the model to the training data\n","start = time.time()\n","classifier.fit(x_train, y_train)\n","elapsed = time.time() - start\n","\n","print(f\"Training time: {round(elapsed)} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btGJ7BTy57_I"},"outputs":[],"source":["# Save the model\n","classifier.save(os.path.join(results_dir['checkpoints'], f'{best_model_name}.model'))"]},{"cell_type":"markdown","metadata":{"id":"-MA_dMhX57_I"},"source":["## Performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2I7AG0h757_I"},"outputs":[],"source":["# Load the best model\n","best_model_path = os.path.join(results_dir['checkpoints'], f'{best_model_name}.model')\n","classifier = NeuralNetworkClassifier.load(best_model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HgHkgLFA57_I"},"outputs":[],"source":["# evaluate the model on the training and testing data\n","train_accuracy = classifier.score(x_train, y_train)\n","test_accuracy = classifier.score(x_test, y_test)\n","print(f\"Train accuracy: {train_accuracy}\")\n","print(f\"Test accuracy: {test_accuracy}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t2LwTfcs57_J"},"outputs":[],"source":["# predict labels\n","y_predict = classifier.predict(x_test)\n","\n","# visualize results\n","valid_df = pd.DataFrame(x_test, columns=columns[:-1])\n","valid_df[\"is_correct\"] = y_test == y_predict\n","sns.pairplot(valid_df, hue=\"is_correct\", vars=valid_df.columns[:-1])\n","plt.savefig(os.path.join(results_dir['figures'], f'pairplot_{best_model_name}.png'))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_W0TtNvN57_J"},"outputs":[],"source":["# compute confusion matrix\n","target_names = [\"Class 0\", \"Class 1\"]\n","confusion = confusion_matrix(y_test, y_predict)\n","df_cm = pd.DataFrame(confusion, index=target_names, columns=target_names)\n","\n","# plot confusion matrix\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n","plt.xlabel(\"Prediction\")\n","plt.ylabel(\"Ground Truth\")\n","plt.title(\"Confusion Matrix\")\n","plt.savefig(os.path.join(results_dir['figures'], f'confusion_matrix_{best_model_name}.png'))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nBRDC6KQ57_J"},"outputs":[],"source":["# Performance metrics\n","print(\"Classification report:\")\n","print(classification_report(y_test, y_predict, target_names=target_names, zero_division=0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ftD4iXHq57_J"},"outputs":[],"source":["average = None\n","metrics = calculate_metrics(confusion, average=average)\n","metrics = {k: np.round(v, 4) for k, v in metrics.items()} # Round the values to 4 decimal places\n","\n","print(f\"Averaging method: {average}\")\n","metrics_df = pd.DataFrame(metrics, index=target_names)\n","print(metrics_df.T) # transpose"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9bovpDJ057_J"},"outputs":[],"source":["average = 'macro'\n","metrics = calculate_metrics(confusion, average=average)\n","metrics = {k: np.round(v, 4) for k, v in metrics.items()} # Round the values to 4 decimal places\n","\n","# Display the performance metrics\n","print(f\"Averaging method: {average}\")\n","for metric, value in metrics.items():\n","    print(f\"{metric}: {value:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"XAYtFuVo57_J"},"source":["## Save Metrics to CSV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vxNb5j_57_J"},"outputs":[],"source":["# Save the parameters and metrics to a CSV file\n","columns = ['Model Name', 'Epochs', 'Feature Map Reps', 'Ansatz Reps'] + list(metrics.keys())\n","data = [model_name, epochs, best_feature_map_reps, best_ansatz_reps] + list(metrics.values())\n","metrics_df = pd.DataFrame([data], columns=columns)\n","metrics_df.to_csv(os.path.join(results_dir['metrics'], f'metrics_{model_name}.csv'), index=False)\n","print(metrics_df)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}